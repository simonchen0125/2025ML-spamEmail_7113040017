# Spam SMS Classification Project

Phase 1 delivers a baseline TF‑IDF + Linear SVM classifier for SMS spam detection. Phase 2 expands the pipeline with additional models and richer tooling.

## Demo Site
[https://2025ml-spamemail7113040017.streamlit.app/](https://2025ml-spamemail7113040017.streamlit.app/)

## Environment Setup

```bash
python -m venv .venv
.\.venv\Scripts\activate  # Windows
pip install -r spam_classifier/requirements.txt
```

## Phase 1 Workflow (SVM Baseline)

1. **Data Ingestion**
   ```bash
   python spam_classifier/data_ingestion.py
   ```
   Downloads the dataset to `spam_classifier/dataset/sms_spam.csv`.

2. **Preprocessing**
   ```bash
   python spam_classifier/preprocess_data.py
   ```
   Cleans messages, fits TF‑IDF, and saves artifacts to `spam_classifier/artifacts/`:
   - `tfidf_features.npz`
   - `labels.csv`
   - `tfidf_vectorizer.pkl`

3. **Train & Evaluate SVM**
   ```bash
   python spam_classifier/train_svm_model.py
   ```
   Trains the LinearSVC baseline, prints accuracy / precision / recall / F1, shows a confusion matrix, and saves:
   - `spam_classifier/artifacts/svm_model.pkl`
   - (Vectorizer overwrites `tfidf_vectorizer.pkl`)

4. **Predict via CLI**
   ```bash
   python spam_classifier/predict_cli.py "Free entry in 2 a wkly comp to win!"
   python spam_classifier/predict_cli.py  # read messages from stdin
   ```
   Loads the saved model/vectorizer and outputs predictions with decision scores.

## Phase 2 Preview (Additional Models)

Ensure preprocessing artifacts exist (run steps above) and then experiment with additional models:

```bash
# Logistic Regression
python spam_classifier/train_logistic_regression.py

# Multinomial Naive Bayes
python spam_classifier/train_naive_bayes.py

# Soft-voting ensemble (LogReg + Naive Bayes)
python spam_classifier/train_ensemble.py
```

Each script prints evaluation metrics and saves its trained model into `spam_classifier/artifacts/`.

### Model Comparison (SVM vs Logistic Regression)

| Metric                     | SVM (LinearSVC) | Logistic Regression |
|----------------------------|-----------------|---------------------|
| Accuracy                   | 0.9830          | 0.9794              |
| Spam Precision             | 0.99            | 0.94                |
| Spam Recall                | 0.89            | 0.90                |
| Spam F1                    | 0.93            | 0.92                |
| Ham Recall                 | 1.00            | 0.99                |
| Confusion Matrix (Spam FN) | 17              | 15                  |

*Observations:* The SVM delivers slightly higher overall accuracy and spam precision, while logistic regression improves spam recall (fewer false negatives). Choose the model that matches your tolerance for false positives vs. missing spam.

## Streamlit App

Launch the interactive interface after generating artifacts:

```bash
streamlit run app.py
```

Features:
- Title/description in Traditional Chinese with English controls.
- Model selector to switch between SVM, logistic regression, Naive Bayes, or the ensemble (when artifacts exist).
- Single-message prediction with decision score or probability.
- Batch predictions via multiline input or CSV upload (`message` column).
- Sidebar shows evaluation metrics and confusion matrices for available models.
- Probability/decision-score histogram for batch predictions.
- Dataset exploration: class distribution, length histogram, and top TF‑IDF tokens for spam vs. ham.

## Repository Structure

```
spam_classifier/
├── artifacts/                  # Generated by preprocessing/training scripts
├── dataset/                    # Holds sms_spam.csv after ingestion
├── data_ingestion.py           # Step 1: download dataset
├── preprocess_data.py          # Step 2: clean, vectorize, save artifacts
├── predict_cli.py              # CLI for classifying new messages
├── train_logistic_regression.py
├── train_naive_bayes.py
├── train_ensemble.py
├── train_svm_model.py          # Step 3: train + eval SVM baseline
├── data_loader.py              # Shared dataset helpers
├── preprocessor.py             # Text cleaning + TF‑IDF logic
├── trainer.py                  # Shared training/evaluation utilities
├── paths.py                    # Centralized filenames/directories
└── requirements.txt
```

## Notes
- Scripts rely on locally stored artifacts to avoid repeated downloads.
- Logistic regression uses balanced class weights; ensemble voting averages probabilities from logistic regression + Naive Bayes.
- Update `openspec` specs/tasks as you refine future phases.
